# ğŸ¡ House Price Prediction â€“ End-to-End ML System

An end-to-end **production-ready Machine Learning project** that predicts house prices using structured housing data.  
The project covers the **complete ML lifecycle** â€” from data ingestion and feature engineering to model training, inference pipelines, and a FastAPI prediction service.

This project is designed with **real-world ML engineering practices**, not just notebooks.

---

##  Project Highlights

- End-to-end **training + inference pipelines**
- Modular, reusable **ML architecture**
- Handles **partial user input** intelligently
- Full **FastAPI inference service**
- Production-ready **sklearn Pipeline**
- Robust **logging, exception handling, and config management**

---

## Problem Statement

Predict the **sale price of a house** based on numerical and categorical features such as:
- Property size
- Quality indicators
- Location (Neighborhood)
- Year built / renovated
- Garage and basement features

---

## How to Run & Test the Project (Quick Start)
1. Train the Model
python pipeline/train_pipeline.py


This will:
Train multiple models
Select the best model
Save the full production pipeline
Store evaluation metrics

2. Start the FastAPI Server
uvicorn app.main:app --reload

3. Swagger UI (API testing):
http://127.0.0.1:8000/docs

4. Test Prediction via API
POST /predict

Sample Payload

{
  "Overall Qual": 7,
  "Gr Liv Area": 1710,
  "Neighborhood": "NridgHt",
  "Garage Cars": 2,
  "Kitchen Qual": "Gd",
  "Exter Qual": "Gd"
}


Sample Response

{
  "predicted_price": 285432.67
}

5. Test Prediction Locally (Without API)
python pipeline/sample_test_prediction.py


## ğŸ—ï¸ Project Architecture

house_price_prediction/
â”‚
â”œâ”€â”€ app/
â”‚ â””â”€â”€ main.py # FastAPI app
â”‚
â”œâ”€â”€ pipeline/
â”‚ â”œâ”€â”€ train_pipeline.py # End-to-end training pipeline
â”‚ â”œâ”€â”€ prediction_pipeline.py # Inference pipeline
â”‚ â””â”€â”€ sample_test_prediction.py
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_ingestion.py
â”‚ â”œâ”€â”€ feature_engineering.py
â”‚ â”œâ”€â”€ outlier_handling.py
â”‚ â”œâ”€â”€ data_preprocessing.py
â”‚ â”œâ”€â”€ encoding.py
â”‚ â”œâ”€â”€ scaling.py
â”‚ â”œâ”€â”€ model_training.py
â”‚ â”œâ”€â”€ model_selection.py
â”‚ â”œâ”€â”€ model_evaluation.py
â”‚ â””â”€â”€ utils/
â”‚ â”œâ”€â”€ logger.py
â”‚ â”œâ”€â”€ exception.py
â”‚ â””â”€â”€ config.py
â”‚
â”œâ”€â”€ artifacts/
â”‚ â”œâ”€â”€ model/
â”‚ â”‚ â”œâ”€â”€ best_model.pkl
â”‚ â”‚ â””â”€â”€ full_pipeline.pkl 
â”‚ â””â”€â”€ reports/
â”‚ â””â”€â”€ model_metrics.json
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/
â”‚ â””â”€â”€ processed/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## âš™ï¸ ML Pipeline Overview

### ğŸ”¹ 1. Data Ingestion
- Loads raw housing data
- Splits into train & test sets
- Saves processed datasets for reproducibility

### ğŸ”¹ 2. Feature Engineering
Custom domain features:
- `House_Age`
- `Remod_Age`
- `Total_Bathrooms`
- `Total_SF`
- Binary indicators (`Has_Garage`, `Has_Basement`)
- Drops redundant columns

---

### ğŸ”¹ 3. Outlier Handling
- IQR-based bounds learned **only from training data**
- Applied consistently during inference

---

### ğŸ”¹ 4. Encoding
- Ordinal encoding for quality-based categorical features
- ColumnTransformer-based architecture

---

### ğŸ”¹ 5. Imputation
- Numerical â†’ median
- Categorical â†’ most frequent
- Ensures inference stability for missing values

---

### ğŸ”¹ 6. Scaling
- StandardScaler applied to numerical features
- Fitted on training data only

---

### ğŸ”¹ 7. Model Training
Trained and evaluated multiple models:
- Linear Regression
- Ridge Regression
- Lasso Regression
- Random Forest Regressor

Metrics tracked:
- Train RÂ²
- Test RÂ²
- RMSE

---

### ğŸ”¹ 8. Model Selection
- Selected based on **test performance + generalization gap**
- Prevents overfitting
- Best model persisted

---

### ğŸ”¹ 9. Full Production Pipeline
All preprocessing + model steps are wrapped into a single **sklearn Pipeline**:

```python
full_pipeline = Pipeline([
    ("feature_engineering", FeatureEngineering()),
    ("outlier_handler", OutlierHandler()),
    ("encoding", encoder),
    ("imputation", preprocessor),
    ("scaling", scaler),
    ("model", best_model)
])
Saved as:
artifacts/model/full_pipeline.pkl
This ensures:
No training/inference skew
One-line .predict() in production

---

## Model Training Results & Evaluation

Multiple regression models were trained and evaluated on the same trainâ€“test split to ensure fair comparison.

### Evaluation Metrics
- **RÂ² Score** â€“ goodness of fit
- **RMSE** â€“ error magnitude
- **Generalization Gap** â€“ |Train RÂ² âˆ’ Test RÂ²|

---

###  Model Performance Comparison

| Model            | Train RÂ² | Test RÂ² | Train RMSE | Test RMSE | Generalization Gap |
|------------------|----------|---------|------------|-----------|--------------------|
| Linear Regression | 0.9282   | 0.8679  | 20,663     | 32,540    | 0.0603             |
| Ridge Regression  | 0.9281   | 0.8719  | 20,673     | 32,047    | 0.0562             |
| Lasso Regression  | 0.9282   | 0.8680  | 20,663     | 32,538    | 0.0602             |
| **Random Forest** â­ | **0.9843** | **0.9265** | **9,656** | **24,282** | **0.0579** |

---

###  Final Model Selection

The **Random Forest Regressor** was selected as the final model because:

- Achieved the **highest Test RÂ² (0.9265)**
- Maintained a **reasonable generalization gap**
- Significantly reduced **RMSE** compared to linear models
- Demonstrated strong non-linear learning capability

To prevent overfitting, model selection logic enforced:
```text
- Maximize Test RÂ²
- Keep generalization gap under control


##  Inference System

ğŸ”¹ Input Adapter
Accepts partial user input
Automatically fills missing features using:
Training medians (numerical)
Training modes (categorical)
This allows realistic user interaction without forcing 80+ inputs.

## Production Features - 

1. Custom exception handling
2. Structured logging
3. Config-driven paths
4. Input validation with Pydantic
5. End-to-end reproducibility


ğŸ“Œ Tech Stack
Python
Pandas, NumPy
Scikit-learn
FastAPI
Pydantic
Joblib

âœ¨ Key Learnings

Designing ML systems beyond notebooks
Handling real-world inference constraints
Avoiding training-serving skew
Writing clean, modular ML code
Building user-facing ML APIs

ğŸ‘¤ Author

Nishi Gupta
Aspiring Machine Learning Engineer